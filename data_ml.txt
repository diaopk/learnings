- Terms
  - Tranining instance
  - training dataset
  - Inductive bias
    - restriction bias: constrains the set of models that the algorithm will consider
      during the learning process
    - preference bias: guides the learning algorithm to perfer certian models over
      others.
  
  - underfitting and overfitting
  - Analytics base table (ABT)

- What is ML
  - In summary, machine learning work by searching through a set of potential models
    to find the prediction model that best generalises beyond the dataset. Machine
    learning alforithms use two sources of informaiton to guide this search, the 
    training dataset and the inductive bias assumed by the algorithm

- CRISP-DM
  - Business understanding
  - Data understanding
  - Data preparation
    - Data quality report: ways to display data through either central tendency or
      data visualisation.
      - Test by e.g. X^2 and ANOVA
    - Outliers handling: clamp transformation, setting threshold
    - Normalisation: used to change the continuous feature to fall within a specifed
      range while maintaining the relative differences between the values for the 
      feature.
      - Binning: define a series of ranges (bins) for the continuous features that
        correspond to the levels fo the new categorical feature we are creaing.
        - Equal-width binning and equal-frequency binning
          - equal-frequency: e.g. 1000 instances and each bin of 100 instances.
        - With deinding different number of bins, we can see the trend or behavouir

    - Sampling
      - Top sampling
      - Random sampling
      - Stratified sampling
        - We want to maintain the stuff like distribution of the target features. 
          This sampling is to ensure the relative frequencies of the levels of a 
          specific stratification feature are maintained in the sampled dataset.
        - How it works is by dividing dataset into a number of stratification or 
          groups that have a particular target feature then randomly select data from
          them to form a sample dataset.
      
      - under-sampling and over-sampling
        - used to get a dataset sample with different relative frequencies of the 
          levels of a particular feature to the distribution in the original dataset.
      - To Test if the sample is good to go

  - Modeling
  - Evaluation
  - Deployment

- Four main families of machine learning algorithms
  - Information-based learning 
  - Similarity based learning
  - Probability based learning
  - Error based learning 
    - Important algorithms
      - gradient descent known as guided search approach
        - randomly select a point from error surface, calculate the slop by 
          determining the derivative of the function used to generate it. Do it again
          to a random point and now we can get the direction where to get the point.
          Keeping processing until the slop is 0.

    - Terms 
      - weight space
      - error surface
      - global minimum of the error surface -> the optimal set of weights (w[0],w[1])
        - The bottom of the error surface has no slop, which the derivate of the func
          is => 0. We can then use differenciate method to get the bottom point 
          such that we can get the optimal weights.

    - simple linear regression model
      - finding the best estimate slop and intercept by computing errors
      - Mw(d) = w[0] + w[1] * d[1]
        where w is the vector(w[0], w[1]), the parameters w[0] and w[1] are refered
        to as weights, d is an instance defined by a single descriptive feature d[1].
        and Mw(d) is the prediction output by the model for the instance d.
      - error function: captures the error between the predictions made by a model 
        and the actual values in a training dataset

      - Example
        size    rental price    model prediction    error   squared error
        500         320               316.47          3.53      12.46
        550         380               347.47          32.53     1058.2
        ...

        L2(Mw, D) = 1/2 * SUM(squared_error) = 2837.08

    - Multivariable Linear Regression.
      - formula
        Mw(d) = w[0]+w[1]*d[1] + w[2]*d[2] + ...
              = w[0] + SUM(w[i]*d[i])

              where w*d is the dot product of vectors w and d. The dot product of two
              vectors is the sum of the products of their corresponding elements.
