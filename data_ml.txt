- Terms
  - Tranining instance
  - training dataset
  - Inductive bias
    - restriction bias: constrains the set of models that the algorithm will consider
      during the learning process
    - preference bias: guides the learning algorithm to perfer certian models over
      others.
  
  - underfitting and overfitting
  - Analytics base table (ABT)

- What is ML
  - In summary, machine learning work by searching through a set of potential models
    to find the prediction model that best generalises beyond the dataset. Machine
    learning alforithms use two sources of informaiton to guide this search, the 
    training dataset and the inductive bias assumed by the algorithm

- CRISP-DM
  - Business understanding
  - Data understanding
  - Data preparation
    - Data quality report: ways to display data through either central tendency or
      data visualisation.
      - Test by e.g. X^2 and ANOVA
    - Outliers handling: clamp transformation, setting threshold
    - Normalisation: used to change the continuous feature to fall within a specifed
      range while maintaining the relative differences between the values for the 
      feature.
      - Binning: define a series of ranges (bins) for the continuous features that
        correspond to the levels fo the new categorical feature we are creaing.
        - Equal-width binning and equal-frequency binning
          - equal-frequency: e.g. 1000 instances and each bin of 100 instances.
        - With deinding different number of bins, we can see the trend or behavouir

    - Sampling
      - Top sampling
      - Random sampling
      - Stratified sampling
        - We want to maintain the stuff like distribution of the target features. 
          This sampling is to ensure the relative frequencies of the levels of a 
          specific stratification feature are maintained in the sampled dataset.
        - How it works is by dividing dataset into a number of stratification or 
          groups that have a particular target feature then randomly select data from
          them to form a sample dataset.
      
      - under-sampling and over-sampling
        - used to get a dataset sample with different relative frequencies of the 
          levels of a particular feature to the distribution in the original dataset.
      - To Test if the sample is good to go

  - Modeling
  - Evaluation
  - Deployment

- Four main families of machine learning algorithms
  - Information-based learning 
  - Similarity based learning
  - Probability based learning
  - Error based learning 
    - Important algorithms
      - gradient descent known as guided search approach
        - randomly select a point from error surface, calculate the slop by 
          determining the derivative of the function used to generate it. Do it again
          to a random point and now we can get the direction where to get the point.
          Keeping processing until the slop is 0.

    - Terms 
      - weight space
      - error surface
      - Cost function
        - is a function maps event or values of one or more variables onto a real 
          number.
        - The lower the result, the more desireable, meaning that the predicted 
          linear regression model has a best fit for the problem.
        - Calculate by:
          1/2m * âˆ‘(Xi-Yi)^2
          where m is the sample size
                xi is the predicted value
                yi is the real value.
                the result is a number.

      - global minimum of the error surface -> the optimal set of weights (w[0],w[1])
        - The bottom of the error surface has no slop, which the derivate of the func
          is => 0. We can then use differenciate method to get the bottom point 
          such that we can get the optimal weights.

    - simple linear regression model
      - finding the best estimate slop and intercept by computing errors
      - Mw(d) = w[0] + w[1] * d[1]
        where w is the vector(w[0], w[1]), the parameters w[0] and w[1] are refered
        to as weights, d is an instance defined by a single descriptive feature d[1].
        and Mw(d) is the prediction output by the model for the instance d.
      - error function: captures the error between the predictions made by a model 
        and the actual values in a training dataset

      - Example
        size    rental price    model prediction    error   squared error
        500         320               316.47          3.53      12.46
        550         380               347.47          32.53     1058.2
        ...

        L2(Mw, D) = 1/2 * SUM(squared_error) = 2837.08

    - Multivariable Linear Regression.
      - formula
        Mw(d) = w[0]+w[1]*d[1] + w[2]*d[2] + ...
              = w[0] + SUM(w[i]*d[i])

              where w*d is the dot product of vectors w and d. The dot product of two
              vectors is the sum of the products of their corresponding elements.

- Types of learings
  - supervised learning 
    - given a dataset containing a label to indicate if the orginal ones are true or
      not (binary variables).
  - unsupervised learning 
    - without giving any labels from the dataset. It's aim is to find the structure
      if the dataset expecially for unstructed data.
    - including finding what 
      - most relevant features
      - hidden parttern

    - used in clustering problem
      - when we have target clustered, we will spend most of stuff on the features.

- Ensamle Methods
  - decision tree
  - Random Forest
  - Ada Boost
    - In contrast with random forest, adaboost is usually just a node and two lefs.
    - a tree will one node and two leaves is called stump
    - In Ada Boost there are forest of stumps.
    - Three ideas behind:
      - combines a lot of 'weak learners' to make clasifications. The weak learners
        are almost aways stumps.
      - Some stumps get more say in the classification than others.
      - Each stump is made by taking the previous mistakes into accounts.
    - It starts with building short tree
    - The amount of say that the stump has on the final output is based on how well
      it compensated for those previous errors.

  - Graident Boost
    - builds fixed szied trees based on the previous tree's errors, but unlike 
      AdaBoost, each tree can be larger than a stump.
    - Scale all the trees by the same amount.
    - Then Gradient Boost builds another tree based on the errors made by the 
      previous tree.
    - The predicted value may be very accurate. In other words, we have low Bias, 
      but probably very high variance. Uses learning rate to deal with this problem
  - 
